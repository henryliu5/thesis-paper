%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{center}
%   \begin{minipage}{0.5\textwidth}
%     \begin{small}
%       In which the reasons for creating this package are laid bare for the
%       whole world to see and we encounter some usage guidelines.
%     \end{small}
%   \end{minipage}
%   \vspace{0.5cm}
% \end{center}

Graphs are increasingly popular and highly expressive structures for representing data.
In the past decade, significant interest in graph analysis has led to the emergence of Graph Neural Networks (GNNs), class of state-of-the-art machine learning methods for graph representation learning. 

GNNs adopt ideas from traditional Deep Neural Networks (DNNs) and combine them with techniques to capture structural information about graphs. Traditional DNNs have excelled in various tasks in areas such as computer vision \cite{AlexNet_2012}\cite{YOLO_2016} and natural language processing \cite{RNN_2013}\cite{NamedEntityRecognition_2016}. In these domains, however, inputs exhibit fairly regular structure, unlike in graphs. 

To bridge the gap between DNNs and graph-structured data, GNNs capture graph structural information by using graph \textit{convolutions}, a technique for aggregating local node neighborhood information. As opposed to traditional graph processing, graphs used with GNNs associate \textit{features} with each node in the graph, which are large multidimensional tensors. GNNs use these features to compute \textit{embeddings} for each node in the graph by recursively aggregating each node's neighboring features. 
The resulting embeddings can be used for tasks such as node classification, link prediction, or graph classification.

% Different GNN architectures such as Graph Convolutional Networks (GCN) \cite{GCN_2016}, GraphSAGE \cite{GraphSAGE_2017}, and Graph Attention Networks (GAT) \cite{GAT_2018} perform different types of aggregations and operations on features, but share the same neighborhood aggregation principle.


GNNs are deployed in production systems for many applications, including bioinformatics \cite{Bioinfo_2021} \cite{Bioinfo_2022}, 
traffic forecasting \cite{Traffic_SST-GNN_2021} \cite{Traffic_GoogleMaps_2021} \cite{Traffic_survey_2021}, recommendation systems \cite{Recsys_PinSAGE_2018} \cite{Recsys_Diffnet_2022}\cite{Recsys_LightGCN_2020}\cite{Recsys_NAGCN_2020}\cite{Recsys_SGL_2021}\cite{Recsys_Survey_2022}, 
cybersecurity \cite{Cybersec_2022} \cite{Cybersec_2023}, and combinatorial optimization \cite{CombinatorialOptimization_2019}\cite{CombinatorialOptimization_2021}, among many others. Although there has been significant work on GNN training systems [todo cite stuff], GNN inference is relatively understudied. 

Many existing GNN inference systems rely on approximate nearest neighbor approaches or periodic offline inference to keep node embeddings fresh \cite{Recsys_PinSAGE_2018} \cite{Recsys_Survey_2022}. However, such approaches may not meet accuracy, latency, or throughput demands of real world systems. For example, in 2013 Facebook's graph database was updated roughly eight-six thousand times per second \cite{Graph_Survey_2020}.

% Thus in this work we look at creating an efficient system for online GNN inference. 
To understand the current state of the art for online GNN inference, we first examine how key optimizations in GNN training systems can apply to GNN inference. One critical optimization we identify is node \textit{feature caching}, the act of storing some node features in GPU memory to avoid redundant host-device transfers.
However, only some caching techniques from GNN training systems are applicable at inference time.
The simplest of these is static degree-based caching, which permanently places features corresponding to nodes with highest out-degree in GPU memory \cite{PaGraph_2020}.
We observe that even when using this static cache, GNN inference still encounters the \textit{data loading} bottleneck, where 30-80\% of inference latency is due to copying node features to the GPU.
Although improving cache hit rate would help alleviate this bottleneck, no GNN training systems implement dynamic caches, even using traditional LRU or LFU cache eviction policies, due to the associated overhead.

We tackle the challenges associated with efficiently implementing dynamic caches in three ways.

First, we motivate the use of dynamic cache policies by noting that GNN inference requests create opportunities to exploit graph locality not present at training time. Assuming mini-batch training, GNNs are trained mini-batches that are generally sampled uniformly randomly from the entire graph. However, at inference time, GNNs need to produce answers for new requests that connect into the existing graph. These requests can exhibit locality in graph structure due to factors such as user trends, leading to "hot" subgraphs.

We then propose a simple frequency-based admission and eviction policy provides better cache hit rates than static caches or traditional policies such as LFU. We demonstrate that this holds across inferences traces corresponding to uniformly sampled requests as well as requests concentrated in hot subgraphs.

Second, we use asynchrony as a technique to avoid overheads due to dynamic cache updates. Although dynamic caches can provide better cache hit rates, end-to-end performance can be eroded due to the overhead of actually performing cache updates. By moving cache update operations to separate host threads and CUDA streams, we take these operations off of the critical path when responding to inference requests. 

Lastly, we propose a lock-free mechanism for performing cache updates. In a system with many inference threads and pipelining, an asynchronous cache update can produce end-to-end performance equivalent to that of a synchronous one if it blocks pipeline stages due to naive locking. Furthermore, our system supports sharing of a single logical cache among multiple GPUs connected by NVLink, and the blocking of inter-GPU communication can exacerbate locking overheads. To address this we show how the cache can be \textit{masked} to allow for wait-free readers and lock-free writers.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Contributions}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this work, we analyze the outsized impact of feature movement from CPU to GPU on GNN inference. 
% Even when using static caching techniques borrowed from GNN training, this data loading step still remains a major bottleneck in achieving low latency inference. Gathering features in host memory and copying to the GPU comprises roughly 50\%-80\% of end-to-end inference latency. We tackle this problem by improving GPU feature caches in the following ways.