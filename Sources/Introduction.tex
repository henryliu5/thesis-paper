%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{center}
%   \begin{minipage}{0.5\textwidth}
%     \begin{small}
%       In which the reasons for creating this package are laid bare for the
%       whole world to see and we encounter some usage guidelines.
%     \end{small}
%   \end{minipage}
%   \vspace{0.5cm}
% \end{center}

Graphs are highly expressive and increasingly popular structures for representing data.
In the past decade, significant interest in graph analysis has led to the emergence of Graph Neural Networks (GNNs), a class of state-of-the-art machine learning methods for graph representation learning. 

GNNs adopt ideas from traditional Deep Neural Networks (DNNs) and combine them with techniques to capture structural information about graphs. Traditional DNNs have excelled in various tasks in areas such as computer vision \cite{AlexNet_2012}\cite{YOLO_2016} and natural language processing \cite{RNN_2013}\cite{NamedEntityRecognition_2016}. In these domains, however, inputs exhibit fairly regular structure, unlike in graphs. 

To bridge the gap between DNNs and graph-structured data, GNNs capture graph structural information by using graph \textit{convolutions}, a technique for aggregating local node neighborhood information. As opposed to traditional graph processing, graphs used with GNNs associate \textit{features} with each node in the graph, which are large multidimensional tensors. GNNs use these features to compute \textit{embeddings} for each node in the graph by recursively aggregating each node's neighboring features and feeding these aggregations into traditional neural networks. 
The resulting embeddings can be used for tasks such as node classification, link prediction, or graph classification.

GNNs have practical applications in many domains, including bioinformatics \cite{Bioinfo_2021} \cite{Bioinfo_2022}, 
traffic forecasting \cite{Traffic_SST-GNN_2021} \cite{Traffic_GoogleMaps_2021} \cite{Traffic_survey_2021}, recommendation systems \cite{Recsys_PinSAGE_2018} \cite{Recsys_Diffnet_2022}\cite{Recsys_LightGCN_2020}\cite{Recsys_NAGCN_2020}\cite{Recsys_SGL_2021}\cite{Recsys_Survey_2022}, 
cybersecurity \cite{Cybersec_2022} \cite{Cybersec_2023}, and combinatorial optimization \cite{CombinatorialOptimization_2019}\cite{CombinatorialOptimization_2021}, among many others. Although there has been significant work on GNN training systems [todo cite stuff], GNN inference is relatively understudied. 

Many existing GNN inference systems rely on approximate nearest neighbor approaches or periodic offline inference to keep node embeddings fresh \cite{Recsys_PinSAGE_2018} \cite{Recsys_Survey_2022}. However, such approaches may not meet accuracy, latency, or throughput demands of real world systems. For example, in 2013, Facebook's graph database was updated roughly eighty-six thousand times per second \cite{Graph_Survey_2020}. Producing up-to-date GNN-generated node embeddings would require similar throughput from a GNN inference system.

% Thus in this work we look at creating an efficient system for online GNN inference. 
To understand the current state of the art for online GNN inference, we first examine how key optimizations in GNN training systems can apply to GNN inference. One critical optimization we identify is node \textit{feature caching}, the act of storing some node features in GPU memory to avoid redundant host-device transfers.
However, only some caching techniques from GNN training systems are applicable at inference time.
The simplest of these is static, degree-based caching, which permanently places features corresponding to the highest out-degree nodes in GPU memory \cite{PaGraph_2020}.
We observe that even when using this static cache approach, GNN inference still encounters a data loading bottleneck, where 30-80\% of inference latency comes from to copying node features to the GPU.
Although improving cache hit rates from dynamic caches would help alleviate this bottleneck, existing GNN training systems have found that traditional cache eviction policies such as LRU or LFU have unacceptable overheads.

We tackle the challenges associated with efficiently implementing dynamic caches in three ways.

First, we motivate the use of dynamic cache policies by noting that GNN inference requests create opportunities to exploit graph locality not present at training time. We posit that inference requests may actually have graph locality in practice due to correlation with real world trends. For example, a traffic forecasting application may experience a drastic rise in inference requests around a city center due rapidly shifting traffic patterns during rush hour. Dynamic caches can capture this "hot subgraph" behavior while static caches cannot. We then propose a simple frequency-based admission and eviction policy provides better cache hit rates than static caches or traditional policies such as LFU. We demonstrate that this holds across inferences traces corresponding to uniformly sampled requests as well as requests concentrated in hot subgraphs.

% [todo get rid of what follows, give concrete subgraph example] Assuming mini-batch training, GNNs are trained mini-batches that are generally sampled uniformly randomly from the entire graph. However, at inference time, GNNs need to produce answers for new requests that connect into the existing graph. These requests can exhibit locality in graph structure due to factors such as user trends, leading to "hot" subgraphs.

Second, we use asynchrony as a mechanism to avoid overheads due to dynamic cache updates. Although dynamic caches can provide better cache hit rates, end-to-end performance can be eroded due to the overhead of actually performing cache updates. By moving cache update operations to separate host threads and CUDA streams, we take these operations off of the critical path when responding to inference requests. 

Lastly, we propose a lock-free mechanism for performing cache updates. In a system with many inference threads and pipelining, an asynchronous cache update using naive locking can block future pipeline stages, leading to similar performance to a synchronous cache update. Furthermore, since our system supports sharing of a single logical cache among multiple GPUs connected by NVLink, and the blocking of inter-GPU communication can exacerbate locking overheads. To address this, we show how the cache can be \textit{masked} to allow for wait-free cache readers with a lock-free cache writer.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Contributions}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this work, we analyze the outsized impact of feature movement from CPU to GPU on GNN inference. 
% Even when using static caching techniques borrowed from GNN training, this data loading step still remains a major bottleneck in achieving low latency inference. Gathering features in host memory and copying to the GPU comprises roughly 50\%-80\% of end-to-end inference latency. We tackle this problem by improving GPU feature caches in the following ways.