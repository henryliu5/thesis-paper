%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Graphs are highly expressive and increasingly popular structures for representing data.
In the past decade, significant interest in graph analysis has led to the emergence of Graph Neural Networks (GNNs), a class of state-of-the-art machine learning methods for graph representation learning. 

GNNs adopt ideas from traditional Deep Neural Networks (DNNs) and combine them with techniques to capture structural information about graphs. Traditional DNNs have excelled in various tasks in areas such as computer vision \cite{AlexNet_2012}\cite{YOLO_2016} and natural language processing \cite{RNN_2013}\cite{NamedEntityRecognition_2016}. In these domains, however, inputs exhibit fairly regular structure, unlike in graphs. 

To bridge the gap between DNNs and graph-structured data, GNNs capture graph structural information using graph \textit{convolutions}, a technique for aggregating local node neighborhood information. As opposed to traditional graph processing, graphs used with GNNs have per-node \textit{features}, which are large multidimensional tensors. GNNs use these features to compute \textit{embeddings} for each node by recursively aggregating each node's neighboring features and feeding these aggregations into traditional neural networks. 
The resulting embeddings can be used for tasks such as node classification, link prediction, or graph classification.

GNNs have practical applications in many domains, including bioinformatics \cite{Bioinfo_2021} \cite{Bioinfo_2022}, 
traffic forecasting \cite{Traffic_SST-GNN_2021} \cite{Traffic_GoogleMaps_2021} \cite{Traffic_survey_2021}, recommendation systems \cite{Recsys_PinSAGE_2018} \cite{Recsys_Diffnet_2022}\cite{Recsys_LightGCN_2020}\cite{Recsys_NAGCN_2020}\cite{Recsys_SGL_2021}\cite{Recsys_Survey_2022}, 
cybersecurity \cite{Cybersec_2022} \cite{Cybersec_2023}, and combinatorial optimization \cite{CombinatorialOptimization_2019}\cite{CombinatorialOptimization_2021}, among many others. Although there has been significant work on GNN training systems \cite{P3_2021}\cite{PaGraph_2020}\cite{BGL_2023}\cite{GNNLab_2022}, GNN inference is relatively understudied. 

Many existing GNN inference systems rely on approximate nearest neighbor approaches or periodic offline inference to keep node embeddings fresh \cite{Recsys_PinSAGE_2018} \cite{Recsys_Survey_2022}. However, such approaches may not meet accuracy, latency, or throughput demands of real world systems. For example, in 2013, Facebook's graph database was updated roughly eighty-six thousand times per second \cite{Graph_Survey_2020}. Maintaining up-to-date GNN-generated node embeddings would require similar throughput from a GNN inference system.

% Thus in this work we look at creating an efficient system for online GNN inference. 
An intuitive approach for efficient inference is to borrow optimizations from GNN training systems, but existing approaches are not suitable for inference. One critical optimization we identify is \textit{feature caching}, the act of storing node features in GPU memory to avoid redundant host-device transfers.
The simplest approach to this is static, degree-based caching, which permanently places features corresponding to the highest out-degree nodes in GPU memory \cite{PaGraph_2020}.
We observe that even when using this static cache approach, GNN inference still encounters a data loading bottleneck, where 30-80\% of inference latency comes from copying node features to the GPU.

We tackle the challenges associated with improving feature caches and reducing data loading overheads in three ways.

First, we motivate the use of dynamic cache policies by noting that GNN inference requests create opportunities to exploit graph locality not present at training time. We posit that inference requests may actually have graph locality due to correlation with real world trends. For example, a traffic forecasting application may experience a drastic rise in inference requests around a city center due to rapidly shifting traffic patterns during rush hour. Dynamic caches can capture the "hot" subgraph corresponding to said city center while static caches cannot. Following this observation, we then propose a simple frequency-based admission and eviction policy that provides better cache hit rates than static caches or traditional policies such as LFU. We demonstrate that this holds across inference traces corresponding to uniformly random requests as well as requests concentrated in hot subgraphs.

% [todo get rid of what follows, give concrete subgraph example] Assuming mini-batch training, GNNs are trained mini-batches that are generally sampled uniformly randomly from the entire graph. However, at inference time, GNNs need to produce answers for new requests that connect into the existing graph. These requests can exhibit locality in graph structure due to factors such as user trends, leading to "hot" subgraphs.

Second, we use asynchrony as a mechanism to avoid overheads due to dynamic cache updates. While dynamic caches can have better cache hit rates, existing GNN training systems have found that traditional cache eviction policies such as LRU or LFU have unacceptable overheads \cite{PaGraph_2020} \cite{GNNLab_2022}. Thus, end-to-end performance gains will be eroded by cache update and management overheads. We propose a mechanism that periodically computes a set of \textit{cache candidates} and performs actual cache update operations to separate host threads and CUDA streams, taking these operations off the critical path.

Finally, we propose a lock-free mechanism for performing cache updates. In a highly concurrent system with multiple GPUs and many inference threads, naive locking can potentially lead to poor inference latency and throughput. For example, our system maximizes cache capacity by sharing a single logical cache among multiple GPUs connected by NVLink, and the blocking of inter-GPU communication will lead to increased latencies. Furthermore, a blocking, asynchronous cache update can degenerate into a synchronous one due to contention with other threads. To address this, we show how the cache can be \textit{masked} to allow for wait-free cache readers and lock-free cache writers.

% system with many inference threads and pipelining, an asynchronous cache update using naive locking can block future pipeline stages, leading to similar or worse performance than a synchronous cache update. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Contributions}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this work, we analyze the outsized impact of feature movement from CPU to GPU on GNN inference. 
% Even when using static caching techniques borrowed from GNN training, this data loading step still remains a major bottleneck in achieving low latency inference. Gathering features in host memory and copying to the GPU comprises roughly 50\%-80\% of end-to-end inference latency. We tackle this problem by improving GPU feature caches in the following ways.