%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We implement our design using Deep Graph Library (DGL) \cite{DGL_2019}, a GNN training framework; PyTorch \cite{PyTorch_2019} a tensor operation library, and a mix of Python and C++.
Our current implementation consists of roughly 12,000 SLOC of Python and 1,000 SLOC of C++, and is publicly available here [todo hyperlink].



\begin{enumerate}
    \item \textbf{Persistent pinned memory buffer for feature transfer:}
    \item \textbf{GPU Sampling}
    \item \textbf{CUDA Multi-Process Serivce (MPS)}
\end{enumerate}


GPU sampling
Feature gather pinned buffer
Multi process service

\begin{figure}[h!]
    \centering
    % \includegraphics[width=\textwidth]{stamp_graphs.png}
    
    \caption{[todo add picture of this system]}
    \label{Our system diagram}
\end{figure}    


\section{Limitations}
[TODO move this elsewhere]
Our system currently does not combine new inference requests into the existing graph or retrain the GNN to accommodate for new requests. 
Instead, we look only at GNN computation and investigate how to efficiently compute new embeddings. 
Integrating new nodes into the existing graph and dealing with challenges such as consistency are both orthogonal and out of scope of this work, but would be an interesting and natural extension.
